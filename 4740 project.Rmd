---
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=6, fig.height=4) 
```

```{r}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

#EDA
```{r}
library(ggplot2)
library("readxl")
data<-read_excel("/Users/UnaLiang/Downloads/cancer_data.xlsx")
df <- data.frame(data)
data$Level <- as.factor(df$Level)
magnitude.counts <- table(df$Level)
library(skimr)
#df %>%
#    dplyr::group_by(Level) %>%
#    skim()
    
cor_x<-cor(df[,c(2:24)])
library(caret)
hc<-findCorrelation(cor_x,cutoff = 0.9)
hc

MD <- mahalanobis(df[,c(2:24)],colMeans(df[,c(2:24)]),cov(df[,c(2:24)]))
df$MD <-round(MD,3)
df$outlier_maha <- FALSE
df$outlier_maha[df$MD >50] <- TRUE
nrow(dplyr::filter(df, outlier_maha == TRUE))
df_clean<- df[-which(df$outlier_maha ==TRUE),]
df_clean <-df_clean[,1:25] 
```

#Data visualization
```{r}
par(c(2,2))
#scatter
plot(df$Age,df$Gender,col='red',xlab='Age',ylab='Gender')

#histogram
hist(df$Age,col='blue')

#boxplot
boxplot(df$Age, df$Gender, df$Air.Pollution,df$Alcohol.use,df$Dust.Allergy, df$OccuPational.Hazards, df$Genetic.Risk,df$chronic.Lung.Disease,
        main = '',
        names = c('Age','Gender','Air Pollution','Alcohol use','Dust Allergy','OccuPational Hazards','Genetic Risk','chronic Lung Disease'),
        col = 'green'
)



boxplot(df$Balanced.Diet, df$Obesity, df$Smoking,df$Passive.Smoker,df$Chest.Pain,df$Coughing.of.Blood,df$Fatigue,df$Weight.Loss,
        main = '',
        names = c('Balanced Diet','Obesity','Smoking','Passive Smoker','Chest Pain','Coughing of Blood','Fatigue','Weight Loss'),
        col = 'green'
)

boxplot(df$Shortness.of.Breath, df$Wheezing, df$Swallowing.Difficulty,df$Clubbing.of.Finger.Nails,df$Frequent.Cold,df$Dry.Cough,df$Snoring,
        main = '',
        names = c('Shortness of Breath','Wheezing','Swallowing Difficulty','Clubbing of Finger Nails','Frequent Cold','Dry Cough','Snoring'),
        col = 'green'
)

```
#LDA
```{r}
library(MASS)
cancer_data_new<-as.data.frame(df_clean)
cancer_data_new<-cancer_data_new[,-1]
cancer_data_new$Level<-as.factor(cancer_data_new$Level)
mse<-rep(0,10)
for (i in 1:10) {
  set.seed(i)
  training_index<-sample(1:nrow(cancer_data_new),0.8*nrow(cancer_data_new))
  lda.fit=lda(Level~.,data=cancer_data_new,subset=training_index)
  lda.pred=predict(lda.fit, cancer_data_new[-training_index,])
  names(lda.pred)
  lda.class=lda.pred$class
  table(lda.class,cancer_data_new[-training_index,24])
  mse[i]<-mean((lda.class==cancer_data_new[-training_index,24]))
}
mean(mse)
```

#Multinomial Logistic Regression

```{r}
cancer_data_new<-df_clean[,-1]
cancer_data_new$Level<-as.factor(cancer_data_new$Level)
cancer_data_model<-model.matrix(Level~.,cancer_data_new)[,-1]

require(foreign)
require(nnet)
require(ggplot2)
require(reshape2)

mse1<-rep(0,10)
for (i in 1:10) {
  training_index<-sample(1:nrow(cancer_data_model),0.8*nrow(cancer_data_model))
  cancer_data_new$Level<-relevel(cancer_data_new$Level,ref = "Low")
  ml<-multinom(Level~.,data = cancer_data_new,subset = training_index)
  ml.fit<-predict(ml,cancer_data_new[-training_index,])
  mse1[i]<-mean((ml.fit==apply(cancer_data_new[-training_index,],2,factor))^2)
}
mean(mse1)


```
#Multinomial Logistic Regression with Lasso
```{r}
library(glmnet)
library(ggplot2)
set.seed(1)
training_index<-sample(1:nrow(cancer_data_model),0.8*nrow(cancer_data_model))
cancer_data_train<-cancer_data_model[training_index,]
cancer_data_test<-cancer_data_model[-training_index,]
y_train<-cancer_data_new$Level[training_index]
y_test<-cancer_data_new$Level[-training_index]
cvfit = cv.glmnet(cancer_data_train,y_train,type.measure="class",alpha=1,family="multinomial")
bestlam=cvfit$lambda.min
pred.lasso = predict(cvfit, s = bestlam, newx = cancer_data_test,type = "response")
predicted <- colnames(pred.lasso)[apply(pred.lasso,1,which.max)]
mean(predicted==y_test)
coef(cvfit)
```



#Multinomial Logistic Regression with Ridge
```{r}
cancer_data<-df_clean[,-1]
cancer_data$Level = as.factor(cancer_data$Level)
set.seed(1)
train.index <- sample(1:nrow(cancer_data),0.8*nrow(cancer_data))
cancer_train <- as.matrix(cancer_data[train.index, 1:23])
cancer_te <- as.matrix(cancer_data[-train.index, 1:23])
level_train <- as.matrix(cancer_data[train.index, 24])
level_te <- as.matrix(cancer_data[-train.index, 24])

library(glmnet)

# Ridge
set.seed(1)
cv.out=cv.glmnet(cancer_train,level_train,alpha=0, family = "multinomial") 
ridge.bestlam=cv.out$lambda.min
ridge.mod=glmnet(cancer_train,level_train,alpha=0,lambda=ridge.bestlam, family = "multinomial")
# ridge.coef = coef(ridge.mod)[,1]
pred.ridge = predict(ridge.mod, s=ridge.bestlam, newx = cancer_te, type="response")
pred.level = colnames(pred.ridge)[apply(pred.ridge, 1, which.max)]
mse.ridge = mean((pred.level==level_te)^2)
mse.ridge # = 0.955
```

#PCA and KNN
```{r}
library("factoextra")
library("class")
data<-df_clean
# principal components regression for preprocessing the data
set.seed(1)
data$Level = as.numeric(as.factor(data$Level))

cancer.pr <- prcomp(data[c(2:24)], center = TRUE, scale = TRUE)
summary(cancer.pr)
screeplot(cancer.pr, type = "l", npcs = 15, main = "Screeplot of the first 15 PCs")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
       col=c("red"), lty=5, cex=0.6)
cumpro <- cumsum(cancer.pr$sdev^2 / sum(cancer.pr$sdev^2))
plot(cumpro[0:15], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
abline(v = 9, col="blue", lty=5)
abline(h = 0.88759, col="blue", lty=5)
legend("topleft", legend=c("Cut-off @ PC6"),
       col=c("blue"), lty=5, cex=0.6)


# Test MSE calculated using KNN method
scaling <- cancer.pr$sdev[1:11] * sqrt(nrow(data))
pc<-matrix(0,970,11)
for (i in 1:11) {
  pc[,i] <- rowSums(t(t(sweep(data[,2:24], 2 ,colMeans(data[,2:24]))) * cancer.pr$rotation[,i]) / scaling[i])
}
colnames(pc)<-c("pc1","pc2","pc3","pc4","pc5","pc6","pc7","pc8","pc9","pc10","pc11")
pc<-as.data.frame(pc)

set.seed(1)
training_index<-sample(1:nrow(pc),0.8*nrow(pc))
pc.train = pc[training_index,]
pc.te = pc[-training_index,]

data.train = data[training_index,]
data.te = data[-training_index,]
level.train <-data.train[,25]
level.te <- data.te[,25]

#k=10
knn.pred=knn(pc.train,pc.te,level.train,k=10)

table(knn.pred,level.te)
mean(knn.pred==level.te)

#k=20
knn.pred=knn(pc.train,pc.te,level.train,k=20)

table(knn.pred,level.te)
mean(knn.pred==level.te)

#It appears that if I use k=10, the accuracy is 89.175%. If k increases to 20, then the accuracy is 88.66%. 


```

#KNN
```{r}
#KNN method:

set.seed(1)
##Generate a random number that is 80% of the total number of rows in data set.
ran <- sample(1:nrow(df_clean), 0.8 * nrow(df_clean)) 

##the normalization function is created
nor <-function(x) { (x -min(x))/(max(x)-min(x))   }

##Run normalization on the 2nd-24th columns of data set because they are the predictors
data_norm <- as.data.frame(lapply(df_clean[,c(2:24)], nor))


##extract training set
data_train <- data_norm[ran,]  #X for training data
##extract testing set
data_test <- data_norm[-ran,]  #X for test data

#extract 25th column(Level) of train dataset because it will be used as 'cl' argument in knn function.
data_target_category <- df_clean[ran,25]

##extract 25th column(Level) if test dataset to measure the accuracy
data_test_category <- df_clean[-ran,25]

##load the package class
library(class)
##run knn function
pred <- knn(data_train,data_test,cl=data_target_category,k=20)

##create confusion matrix
tab <- table(pred,data_test_category)

##this function divides the correct predictions by total number of predictions that tell us how accurate the model is.

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)

#In the data, I have run the k-nearest neighbor algorithm that gave me 96.90722% accurate result. 
#Compared to different values of k, I found that the accuracy 96.90722% is good enough under k value equals to 20.

```



